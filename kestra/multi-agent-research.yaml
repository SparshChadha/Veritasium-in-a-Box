id: multi-agent-research
namespace: dev

inputs:
  - id: topic
    type: STRING
    defaults: "The science of why time moves forward"

tasks:
  # Parallel execution of three research agents
  - id: parallel_research
    type: io.kestra.core.tasks.flows.Parallel
    tasks:
      # Agent A: The Historian - Wikipedia + Wikidata
      - id: agent_a_historian
        type: io.kestra.plugin.scripts.python.Script
        allowFailure: false
        beforeCommands:
          - pip install requests
        script: |
          import json
          import requests
          import urllib.parse
          topic = "{{ inputs.topic }}"
          print(f"üèõÔ∏è Historian researching: {topic}")
          try:
            # 1. Wikipedia Summary
            url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{urllib.parse.quote(topic)}"
            headers = {'User-Agent': 'VeritasiumHackathonBot/1.0'}
            response = requests.get(url, headers=headers, timeout=10)
            
            wiki_result = {}
            if response.status_code == 200:
              data = response.json()
              wiki_result = {
                "title": data.get("title", ""),
                "extract": data.get("extract", ""),
                "url": data.get("content_urls", {}).get("desktop", {}).get("page", "")
              }
            else:
              wiki_result = {"error": "Wikipedia page not found"}

            # 2. Wikidata Facts (SPARQL) - FIXED: Safe string building
            label_part = topic + '"@en'  # Build label safely
            sparql_query = """
            SELECT ?item ?itemLabel ?description WHERE {
              ?item rdfs:label "%s" .
              SERVICE wikibase:label { bd:serviceParam wikibase:language "en" . }
            } LIMIT 1
            """ % label_part  # % formatting avoids {} conflicts

            sparql_url = "https://query.wikidata.org/sparql"
            sparql_params = {"query": sparql_query, "format": "json"}

            facts = []
            try:
              sparql_response = requests.get(sparql_url, params=sparql_params, timeout=5)
              if sparql_response.status_code == 200:
                wikidata = sparql_response.json().get("results", {}).get("bindings", [])
                facts = [{"label": i["itemLabel"]["value"], "desc": i.get("description", {}).get("value", "")} for i in wikidata]
            except Exception as e:
              print(f"Wikidata error: {e}")

            result = {
              "agent": "Historian",
              "topic": topic,
              "extract": wiki_result.get("extract", ""),
              "wiki_data": wiki_result,
              "wikidata_facts": facts,
              "status": "success"
            }
          except Exception as e:
            result = {
              "agent": "Historian",
              "topic": topic,
              "error": str(e),
              "status": "failed"
            }
          with open("historian.json", "w") as f:
            json.dump(result, f, indent=2)
        outputFiles:
          - "historian.json"

      # Agent B: The Skeptic - Stack Exchange + NewsAPI
      - id: agent_b_skeptic
        type: io.kestra.plugin.scripts.python.Script
        allowFailure: true
        beforeCommands:
          - pip install requests
        script: |
          import json
          import requests
          import os
          
          topic = "{{ inputs.topic }}"
          print(f"ü§î Skeptic researching: {topic}")
          
          try:
              all_posts = []
              
              # 1. Stack Exchange
              stack_sites = ["physics", "skeptics", "science", "astronomy"]
              for site in stack_sites:
                  try:
                      url = "https://api.stackexchange.com/2.3/search/advanced"
                      params = {
                          "site": site,
                          "q": topic,
                          "sort": "votes",
                          "pagesize": 2,
                          "order": "desc",
                          "filter": "!nNPvSNPH.z"
                      }
                      response = requests.get(url, params=params, timeout=5)
                      if response.status_code == 200:
                          items = response.json().get("items", [])
                          for item in items:
                              all_posts.append({
                                  "source": f"StackExchange ({site})",
                                  "title": item.get("title", ""),
                                  "snippet": item.get("body_markdown", "")[:200] + "...",
                                  "score": item.get("score", 0),
                                  "url": item.get("link", "")
                              })
                  except Exception as e:
                      print(f"StackExchange error ({site}): {e}")
                      continue

              # 2. NewsAPI (Optional)
              news_key = os.environ.get("NEWS_API_KEY")
              if news_key:
                  try:
                      news_url = "https://newsapi.org/v2/everything"
                      news_params = {
                          "apiKey": news_key,
                          "q": f"{topic} AND (myth OR misconception OR study)",
                          "sortBy": "relevancy",
                          "pageSize": 2,
                          "language": "en"
                      }
                      resp = requests.get(news_url, params=news_params, timeout=5)
                      if resp.status_code == 200:
                          articles = resp.json().get("articles", [])
                          for art in articles:
                              all_posts.append({
                                  "source": "NewsAPI",
                                  "title": art.get("title", ""),
                                  "snippet": art.get("description", ""),
                                  "url": art.get("url", "")
                              })
                  except Exception as e:
                      print(f"NewsAPI error: {e}")

              result = {
                  "agent": "Skeptic",
                  "topic": topic,
                  "posts": all_posts,
                  "total_found": len(all_posts),
                  "status": "success"
              }
              
          except Exception as e:
              result = {
                  "agent": "Skeptic",
                  "topic": topic,
                  "error": str(e),
                  "status": "failed"
              }
          
          with open("skeptic.json", "w") as f:
              json.dump(result, f, indent=2)
        outputFiles:
          - "skeptic.json"

      # Agent C: The Professor - Semantic Scholar
      - id: agent_c_professor
        type: io.kestra.plugin.scripts.python.Script
        allowFailure: false
        beforeCommands:
          - pip install requests
        script: |
          import json
          import requests
          
          topic = "{{ inputs.topic }}"
          print(f"üéì Professor researching: {topic}")
          
          try:
              # Semantic Scholar Graph API
              url = "https://api.semanticscholar.org/graph/v1/paper/search"
              params = {
                  "query": topic,
                  "limit": 5,
                  "fields": "title,authors,abstract,year,citationCount,openAccessPdf"
              }
              headers = {"User-Agent": "VeritasiumHackathonBot/1.0"}
              
              response = requests.get(url, params=params, headers=headers, timeout=10)
              
              papers = []
              if response.status_code == 200:
                  data = response.json().get("data", [])
                  for p in data:
                      if p.get("abstract"):
                          papers.append({
                              "title": p.get("title"),
                              "authors": [a["name"] for a in p.get("authors", [])[:2]],
                              "abstract": p.get("abstract")[:300] + "...",
                              "year": p.get("year"),
                              "citations": p.get("citationCount", 0),
                              "pdf_url": p.get("openAccessPdf", {}).get("url") if p.get("openAccessPdf") else None
                          })
              
              # Sort by citations
              papers.sort(key=lambda x: x.get('citations', 0), reverse=True)
              
              result = {
                  "agent": "Professor",
                  "topic": topic,
                  "papers": papers[:3],
                  "total_found": len(papers),
                  "status": "success"
              }
              
          except Exception as e:
              result = {
                  "agent": "Professor",
                  "topic": topic,
                  "error": str(e),
                  "status": "failed"
              }
          
          with open("professor.json", "w") as f:
              json.dump(result, f, indent=2)
        outputFiles:
          - "professor.json"

  # Data Aggregation and Synthesis (Top-Level Task)
  - id: combine_research
    type: io.kestra.plugin.scripts.python.Script
    inputFiles:
      historian.json: "{{ outputs.agent_a_historian.outputFiles['historian.json'] }}"
      skeptic.json: "{{ outputs.agent_b_skeptic.outputFiles['skeptic.json'] }}"
      professor.json: "{{ outputs.agent_c_professor.outputFiles['professor.json'] }}"
    beforeCommands:
      - pip install python-slugify  # For safe filenames
    script: |
      import json
      import os
      from slugify import slugify  # Now installed

      def load_agent_data(filename, agent_name):
        if not os.path.exists(filename):
          return {"agent": agent_name, "status": "failed", "error": "File missing"}
        try:
          with open(filename, 'r') as f:
            return json.load(f)
        except Exception as e:
          return {"agent": agent_name, "status": "error", "error": str(e)}

      historian_data = load_agent_data("historian.json", "Historian")
      skeptic_data = load_agent_data("skeptic.json", "Skeptic")
      professor_data = load_agent_data("professor.json", "Professor")

      combined_research = {
        "topic": "{{ inputs.topic }}",
        "timestamp": "{{ execution.startDate }}",
        "agents": {
          "historian": historian_data,
          "skeptic": skeptic_data,
          "professor": professor_data
        },
        "summary": {
          "historian_status": historian_data.get("status", "unknown"),
          "skeptic_status": skeptic_data.get("status", "unknown"),
          "professor_status": professor_data.get("status", "unknown")
        }
      }

      # FIXED: Relative path for Kestra capture + mount sync
      output_dir = "research_outputs"  # Relative (captured in working dir, synced via mount)
      os.makedirs(output_dir, exist_ok=True)
      output_file = f"{output_dir}/kestra_output.json"
      stable_file = "combined_research.json"  # Stable name to make downstream copying easy

      print(f"DEBUG: Writing to {output_file}")  # Log path

      try:
        with open(output_file, "w") as f:
          json.dump(combined_research, f, indent=2, ensure_ascii=False)
        print(f"SUCCESS: Saved {output_file}")
        with open(stable_file, "w") as f:
          json.dump(combined_research, f, indent=2, ensure_ascii=False)
        print(f"SUCCESS: Saved {stable_file}")
      except Exception as e:
        print(f"ERROR saving file: {e}")

      print("="*80)
      print("TRIANGLE OF TRUTH - COMBINED RESEARCH OUTPUT")
      print("="*80)
      print(f"Topic: {{ inputs.topic }}")
      print(f"Slug: {topic_slug}")
      print(f"Historian: {historian_data.get('status', 'unknown')}")
      print(f"Skeptic: {skeptic_data.get('status', 'unknown')}")
      print(f"Professor: {professor_data.get('status', 'unknown')}")
      print("="*80)
    outputFiles:
      - "research_outputs/*.json"  # Captures all
      - "combined_research.json"

  # Persist the combined JSON to a host-mounted folder (works when Kestra runs in Docker)
  - id: persist_combined_to_host
    type: io.kestra.plugin.scripts.python.Script
    inputFiles:
      combined_research.json: "{{ outputs.combine_research.outputFiles['combined_research.json'] }}"
    beforeCommands:
      - pip install python-slugify
    script: |
      import json
      import os
      from slugify import slugify

      # This path is bind-mounted in docker-compose.yml:
      #   ./research_outputs:/app/research_outputs
      host_mounted_dir = os.environ.get("HOST_OUTPUT_DIR", "/app/research_outputs")
      os.makedirs(host_mounted_dir, exist_ok=True)

      out_path = os.path.join(host_mounted_dir, "kestra_output.json")

      with open("combined_research.json", "r") as f:
        payload = json.load(f)

      with open(out_path, "w") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)

      print(f"SUCCESS: persisted to host-mounted path: {out_path}")